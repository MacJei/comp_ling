{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lnpetrova/comp_ling/blob/master/hw03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juPmIHBckX-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3NEyW5YkoW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gzip\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8NZas13NE6K",
        "colab_type": "code",
        "outputId": "b75afda3-59ba-436c-b7f6-645a6f5b50ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-25 08:27:25--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123167 (120K) [text/plain]\n",
            "Saving to: ‘sents_with_mistakes.txt’\n",
            "\n",
            "\rsents_with_mistakes   0%[                    ]       0  --.-KB/s               \rsents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-25 08:27:25 (5.19 MB/s) - ‘sents_with_mistakes.txt’ saved [123167/123167]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBFGMAk7NRUn",
        "colab_type": "code",
        "outputId": "fd0ac91c-7265-4566-88a0-d21a22996ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-25 08:27:29--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120672 (118K) [text/plain]\n",
            "Saving to: ‘correct_sents.txt’\n",
            "\n",
            "\rcorrect_sents.txt     0%[                    ]       0  --.-KB/s               \rcorrect_sents.txt   100%[===================>] 117.84K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-25 08:27:29 (5.20 MB/s) - ‘correct_sents.txt’ saved [120672/120672]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhqTrMj6NoML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO2sFWmvNrGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK7PZaSuQUBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alignes = [align_words(g, b) for g, b in zip(true, bad)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN9GYs9mAj-d",
        "colab_type": "code",
        "outputId": "0c2e2384-ed7a-4501-fd06-18f8cbc615a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-25 08:27:33--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191125T082734Z&X-Amz-Expires=300&X-Amz-Signature=71f4df775bf9a1b67c30f00a8c037a0ef86feb9cebdc65b1094948aa20929063&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-11-25 08:27:34--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20191125%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191125T082734Z&X-Amz-Expires=300&X-Amz-Signature=71f4df775bf9a1b67c30f00a8c037a0ef86feb9cebdc65b1094948aa20929063&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.107.68\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.107.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  41.6MB/s    in 13s     \n",
            "\n",
            "2019-11-25 08:27:47 (38.7 MB/s) - ‘lenta-ru-news.csv.gz’ saved [527373240/527373240]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13Eaky3ZOK-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open('corpus_5000.txt', 'w')\n",
        "with gzip.open('lenta-ru-news.csv.gz', 'rt') as archive:\n",
        "  reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
        "  for i, line in enumerate(reader):\n",
        "    if i < 5000: \n",
        "      corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoLuz8O00ANn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaqonqWnO20g",
        "colab_type": "code",
        "outputId": "5255eca3-8d84-4d3b-c485-49bca108a547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC8E0lH90ANr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naNfqmLW0AOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)\n",
        "\n",
        "#print(WORDS.most_common())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q5yAR-ja-X1l",
        "colab": {}
      },
      "source": [
        "def deletion(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    return set(deletes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XhN3t98i5zV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "second_dictionary = defaultdict(list)\n",
        "for word in WORDS: \n",
        "  forms = deletion(word) \n",
        "  for form in forms:\n",
        "    second_dictionary[form].append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCRSXFfXcFEb",
        "colab_type": "code",
        "outputId": "1ce49dfa-2d02-43aa-dc98-530635764b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(list(second_dictionary.items())[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('txt', ['text']), ('tet', ['text']), ('tex', ['text']), ('ext', ['text', 'next']), ('вице-премье', ['вице-премьер']), ('вице-премьр', ['вице-премьер']), ('ице-премьер', ['вице-премьер']), ('виц-премьер', ['вице-премьер']), ('вицепремьер', ['вице-премьер']), ('вице-преьер', ['вице-премьер']), ('вице-ремьер', ['вице-премьер']), ('вице-пемьер', ['вице-премьер']), ('вие-премьер', ['вице-премьер']), ('вце-премьер', ['вице-премьер']), ('вице-прмьер', ['вице-премьер']), ('вице-премер', ['вице-премьер']), ('о', ['по', 'от', 'об', 'он', 'со', 'во', 'то', 'до', 'но', 'ко', 'бо', 'ао', 'оп', 'ок', 'ос', 'оф', 'яо', 'мо', 'го', 'ой', '\\u200eо', 'оз', 'ох', 'ои', 'чо', 'фо']), ('п', ['по', 'мп', 'кп', 'чп', 'ип', 'еп', 'оп', 'нп', 'сп', 'пи']), ('сциальным', ['социальным']), ('социальым', ['социальным']), ('социаьным', ['социальным']), ('социальны', ['социальным', 'социальные', 'социальных', 'социальный']), ('социльным', ['социальным']), ('оциальным', ['социальным']), ('социалным', ['социальным']), ('соцальным', ['социальным']), ('соиальным', ['социальным']), ('социальнм', ['социальным', 'социальном']), ('опросам', ['вопросам']), ('впросам', ['вопросам']), ('вопрсам', ['вопросам']), ('вопросм', ['вопросам', 'вопросом']), ('вопосам', ['вопросам']), ('вопроса', ['вопросам', 'вопросах']), ('воросам', ['вопросам']), ('вопроам', ['вопросам']), ('атьяна', ['татьяна', 'катьяна']), ('ттьяна', ['татьяна']), ('татьна', ['татьяна']), ('татяна', ['татьяна']), ('татьян', ['татьяна', 'татьяне', 'татьяну', 'татьяны']), ('татьяа', ['татьяна']), ('таьяна', ['татьяна']), ('голиова', ['голикова']), ('голиква', ['голикова']), ('оликова', ['голикова']), ('гликова', ['голикова']), ('голикоа', ['голикова']), ('гоикова', ['голикова']), ('голкова', ['голикова']), ('голиков', ['голикова']), ('расскзала', ['рассказала']), ('рссказала', ['рассказала']), ('расскаала', ['рассказала']), ('рассазала', ['рассказала']), ('рассказаа', ['рассказала', 'рассказана']), ('рассказла', ['рассказала']), ('ассказала', ['рассказала']), ('рассказал', ['рассказала', 'рассказали', 'рассказало']), ('расказала', ['рассказала']), ('', ['в', 'и', 'а', 'с', 'к', 'о', '3', 'б', '4', 'у', '5', '1', '6', '7', 'i', '8', '–', 'я', '9', '2', 'c', 'g', 'ж', 'e', 'f', 'h', 'd', 'п', 'j', 'b', 'x', 'm', '災', '平', '北', 'q', 'х', 'э', 'a', '•', '🤢', 'ш', 'v', '№', 'д', 'е', 'г', 'з', 'ц', '\\u200f', 'n', 'u', 'р', 'т', 'ч', 'k', '📷', '🎥', '\\U0001f970', '👑', '\\U0001f92a', 'ю', 'o', 'м', '™', 's', 'l', 'r', '😂', '💛', '💎', '\\U0001f932', '😝', '😋', '😎', 't', '😯', '✨', '0']), ('аких', ['каких', 'таких']), ('каих', ['каких']), ('кких', ['каких']), ('какх', ['каких']), ('каки', ['каких', 'какие', 'каким', 'кашки', 'каски']), ('регинах', ['регионах']), ('региона', ['регионах', 'регионам']), ('регонах', ['регионах']), ('регионх', ['регионах']), ('ргионах', ['регионах']), ('реионах', ['регионах']), ('егионах', ['регионах']), ('региоах', ['регионах']), ('рссии', ['россии']), ('росии', ['россии']), ('росси', ['россии', 'россию', 'россия']), ('оссии', ['россии']), ('зафиксироваа', ['зафиксирована', 'зафиксировала']), ('зфиксирована', ['зафиксирована']), ('зафксирована', ['зафиксирована']), ('зафиксирован', ['зафиксирована', 'зафиксированы', 'зафиксировано']), ('зафиксировна', ['зафиксирована']), ('зафисирована', ['зафиксирована']), ('зафиксрована', ['зафиксирована']), ('зафиксирвана', ['зафиксирована']), ('зафиксиована', ['зафиксирована']), ('зафиксироана', ['зафиксирована']), ('зафикирована', ['зафиксирована']), ('заиксирована', ['зафиксирована']), ('афиксирована', ['зафиксирована']), ('наиблее', ['наиболее']), ('наиолее', ['наиболее']), ('наибоее', ['наиболее']), ('наболее', ['наиболее']), ('наиболе', ['наиболее']), ('ниболее', ['наиболее']), ('аиболее', ['наиболее']), ('высоая', ['высокая']), ('высокя', ['высокая'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVb_42hOgcn1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXWoodvwQeq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_mistakes(token):\n",
        "  candidates = []\n",
        "  if token in WORDS:\n",
        "     candidates.append(token)\n",
        "  else:\n",
        "    if token in second_dictionary:\n",
        "      candidates += list(second_dictionary[token])\n",
        "    if token not in second_dictionary:\n",
        "      tokens_with_deletion = list(deletion(token))\n",
        "      for token_with_deletion in tokens_with_deletion:\n",
        "        if token_with_deletion in WORDS:\n",
        "          candidates.append(token_with_deletion)\n",
        "        if token_with_deletion in second_dictionary:\n",
        "          candidates += second_dictionary[token_with_deletion]\n",
        "  \n",
        "  if not candidates:\n",
        "    candidates.append(token)\n",
        "  return candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd0curS0ckSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = sum(WORDS.values())\n",
        "def probability(word, N=N): \n",
        "    return WORDS[word] / N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNZEkfbyDDAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_correct(correct_mistakes):\n",
        "    probabilities = {var:probability(var) for var in correct_mistakes}\n",
        "    return max(probabilities, key = probabilities.get)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trMx7zm7KBIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corrected_sent(sent):\n",
        "  corrected = []\n",
        "  for token in sent:\n",
        "    corrected.append(choose_correct(correct_mistakes(token)))\n",
        "  return corrected     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSPFgFla6Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corrected_sents = []\n",
        "for sent in [[token[1] for token in align] for align in alignes]:\n",
        "  corrected_sents.append(corrected_sent(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ2lUz9PfGPP",
        "colab_type": "text"
      },
      "source": [
        "# Ngrams\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwzsLH5jfbUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS['<start>'] = 1\n",
        "WORDS['<end>'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoP4NLZrfhmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb981a41-4465-49a2-a26d-96893582e503"
      },
      "source": [
        "def ngrammer(tokens, n=3):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<function ngrammer at 0x7f559f0eb620>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9q2sxsOfrPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trigrams = Counter()\n",
        "for sent in [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]:\n",
        "    trigrams.update(ngrammer(sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9khYQBejf3FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams = Counter()\n",
        "for sent in [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]:\n",
        "    bigrams.update(ngrammer(sent, n=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI2fc7uHf_eS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def probability_trigram(trigram):\n",
        "    probability_trigram = trigrams[trigram]/bigrams[:2]\n",
        "    return probability_trigram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1StsaOG3CZwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b61f4951-0c1b-4281-8784-faf76fc81e22"
      },
      "source": [
        "print(list(bigrams.items())[:10])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('<start>', '<start>'), 59152), (('<start>', 'text'), 1), (('text', '<end>'), 1), (('<start>', 'вице-премьер'), 12), (('вице-премьер', 'по'), 6), (('по', 'социальным'), 4), (('социальным', 'вопросам'), 2), (('вопросам', 'татьяна'), 2), (('татьяна', 'голикова'), 4), (('голикова', 'рассказала'), 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL7RlUjsC4my",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_trigram(ngram, candidates):\n",
        "    probabilities = {}\n",
        "    trigrams = [(ngram[0], ngram[1], var) for var in list(candidates)]\n",
        "    for trigram in trigrams:\n",
        "        if trigram[:2] in bigrams:\n",
        "            probability_trigram = trigrams[trigram]/bigram[trigram[:2]]\n",
        "            if probability_trigram != 0: \n",
        "                probabilities[trigram[-1]] = probability_trigram\n",
        "    \n",
        "    if not probabilities: \n",
        "        probabilities = {var:probability(var) for var in list(candidates)}    \n",
        "    \n",
        "    return max(probabilities, key=probabilities.get)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2eqkoxjgbby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corrected_sent_with_trigrams(sent):\n",
        "  corrected_sent = []\n",
        "  ngrams = ngrammer(['<start>', '<start>'] + sent + ['<end>'])\n",
        "  for ngram in ngrams:\n",
        "    if ngram[-1] in WORDS:\n",
        "      corrected_sent.append(ngram[-1])\n",
        "    else:\n",
        "      token = ngram[-1]\n",
        "      candidates = correct_mistakes(token)\n",
        "      if candidates:\n",
        "        candidates.append(choose_trigram(ngram, candidates))\n",
        "      else:\n",
        "        corrected_sent.append(token)\n",
        "    print(corrected_sent)\n",
        "    return corrected_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGTznyK1LzYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "333a8f0e-d4c1-4aee-ef28-fc53dddf31dc"
      },
      "source": [
        "corrected_sents = []\n",
        "for sent in [[token[1] for token in align] for align in alignes]:\n",
        "  corrected_sents.append(corrected_sent_with_trigrams(sent))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-a8542a425fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorrected_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malign\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malignes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mcorrected_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected_sent_with_trigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-135-009554ac7202>\u001b[0m in \u001b[0;36mcorrected_sent_with_trigrams\u001b[0;34m(sent)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_mistakes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoose_trigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcorrected_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-131-7845da0fc055>\u001b[0m in \u001b[0;36mchoose_trigram\u001b[0;34m(ngram, candidates)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrigram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mprobability_trigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrigrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprobability_trigram\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobability_trigram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
          ]
        }
      ]
    }
  ]
}